{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a351bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee458ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "import json\n",
    "\n",
    "class TavilyToolFlexible(BaseTool):\n",
    "    name: str = \"tavily_search_results_json\"\n",
    "    description: str = \"Query Tavily and return raw content for LLM to handle.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "        results = client.search(query=query, max_results=1)\n",
    "\n",
    "        try:\n",
    "            content = results[\"results\"][0][\"content\"]\n",
    "            # Nếu content là JSON lồng dạng chuỗi → parse để dễ đọc\n",
    "            try:\n",
    "                parsed = json.loads(content)\n",
    "                return json.dumps(parsed, indent=2)  # đẹp hơn, dễ đọc với LLM\n",
    "            except json.JSONDecodeError:\n",
    "                return content  # content không phải JSON → trả nguyên\n",
    "        except Exception as e:\n",
    "            return f\"Error while parsing Tavily response: {e}\"\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "tools = [TavilyToolFlexible()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a6539e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tools = [\n",
    "    TavilySearchResults(max_results=3,\n",
    "    api_key=os.getenv(\"TAVILY_API_KEY\"))]\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_executor = ToolNode(tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama3-70b-8192\",\n",
    "     temperature=0,api_key=os.getenv(\"GROQ_API_KEY\") \n",
    "     )\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43947ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c356d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentAction\n",
    "\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Hỗ trợ cả GPT 3.5/4 (function_call) và GPT-4o/Groq (tool_calls)\n",
    "    if \"function_call\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    elif \"tool_calls\" in last_message.additional_kwargs:\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "    \n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\" : [response]}\n",
    "\n",
    "\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]  # lấy call đầu tiên\n",
    "    tool_name = tool_call[\"function\"][\"name\"]\n",
    "    tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "    action = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=\"\",  # để trống nếu không cần\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=tool_name)\n",
    "    return {\"messages\": [function_message]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ff93006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]  # lấy call đầu tiên\n",
    "    tool_name = tool_call[\"function\"][\"name\"]\n",
    "    tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "    action = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=\"\",  # để trống nếu không cần\n",
    "    )\n",
    "    response = input(prompt=f\"[y/n] continue with: {action}?\")\n",
    "    if response == \"n\":\n",
    "        raise ValueError\n",
    "    \n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=tool_name)\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e24b05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a80fcb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_c4xa', 'function': {'arguments': '{\"query\":\"current weather in san francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 945, 'total_tokens': 997, 'completion_time': 0.166850143, 'prompt_time': 0.032704355, 'queue_time': 0.22790129100000003, 'total_time': 0.199554498}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d7ee8c96-2491-4b28-a76a-dd2d5b30a1e6-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in san francisco'}, 'id': 'call_c4xa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 945, 'output_tokens': 52, 'total_tokens': 997})]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'action':\n",
      "------------------\n",
      "{'messages': [FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json')]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content=\"It seems like the tool_calls didn't yield the desired result. In this case, I'll respond directly without using a tool.\\n\\nThe current weather in San Francisco is typically mild and cool, with foggy mornings and sunny afternoons. However, I'd recommend checking a weather website or app for the most up-to-date and accurate information.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 1012, 'total_tokens': 1082, 'completion_time': 0.290956809, 'prompt_time': 0.044094908, 'queue_time': 0.22378563900000004, 'total_time': 0.335051717}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--a7962ac8-41c1-4f0d-aba6-f78dc03f1c3c-0', usage_metadata={'input_tokens': 1012, 'output_tokens': 70, 'total_tokens': 1082})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from node '{key}':\")\n",
    "        print(\"------------------\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cc9facb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the weather in london?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_e2yf', 'function': {'arguments': '{\"query\":\"weather in london\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 917, 'total_tokens': 966, 'completion_time': 0.161035886, 'prompt_time': 0.029220172, 'queue_time': 0.22545678400000002, 'total_time': 0.190256058}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--b966da00-0db9-45b2-b4c6-dd56028bdafb-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in london'}, 'id': 'call_e2yf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 917, 'output_tokens': 49, 'total_tokens': 966}),\n",
       "  FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_73er', 'function': {'arguments': '{\"query\":\"london weather forecast\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 982, 'total_tokens': 1022, 'completion_time': 0.114285714, 'prompt_time': 0.031408488, 'queue_time': 0.22713646799999998, 'total_time': 0.145694202}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--083302e5-5eee-4e58-b12b-047b0e4bc9d0-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'london weather forecast'}, 'id': 'call_73er', 'type': 'tool_call'}], usage_metadata={'input_tokens': 982, 'output_tokens': 40, 'total_tokens': 1022}),\n",
       "  FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json'),\n",
       "  AIMessage(content='I apologize, but it seems that the tools I provided are not yielding the desired results. In this case, I will respond directly without using a tool.\\n\\nThe weather in London is subject to change, but I can suggest some ways for you to find the current weather in London. You can check online weather websites such as AccuWeather, BBC Weather, or Google Weather.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 1047, 'total_tokens': 1123, 'completion_time': 0.350235146, 'prompt_time': 0.033469426, 'queue_time': 0.219757772, 'total_time': 0.383704572}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--d15b4250-47e4-4716-bdd0-fd42ed9607a3-0', usage_metadata={'input_tokens': 1047, 'output_tokens': 76, 'total_tokens': 1123})]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What is the weather in london?\")]}\n",
    "app.invoke(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Tải API key từ file .env\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "client = TavilyClient()\n",
    "\n",
    "# Gửi truy vấn tìm kiếm\n",
    "result = client.search(query=\"London weather forecast\", max_results=1)\n",
    "\n",
    "# In kết quả\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "759b5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'London weather forecast', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in London', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'London', 'region': 'City of London, Greater London', 'country': 'United Kingdom', 'lat': 51.5171, 'lon': -0.1062, 'tz_id': 'Europe/London', 'localtime_epoch': 1746352007, 'localtime': '2025-05-04 10:46'}, 'current': {'last_updated_epoch': 1746351900, 'last_updated': '2025-05-04 10:45', 'temp_c': 11.3, 'temp_f': 52.3, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 11.4, 'wind_kph': 18.4, 'wind_degree': 20, 'wind_dir': 'NNE', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.01, 'precip_in': 0.0, 'humidity': 58, 'cloud': 50, 'feelslike_c': 9.2, 'feelslike_f': 48.5, 'windchill_c': 9.8, 'windchill_f': 49.6, 'heatindex_c': 11.8, 'heatindex_f': 53.2, 'dewpoint_c': 1.1, 'dewpoint_f': 34.0, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 2.7, 'gust_mph': 13.1, 'gust_kph': 21.1}}\", 'score': 0.9706119, 'raw_content': None}], 'response_time': 1.59}\n"
     ]
    }
   ],
   "source": [
    "from tavily import TavilyClient\n",
    "client = TavilyClient()\n",
    "result = client.search(query=\"London weather forecast\", max_results=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607b1ab",
   "metadata": {},
   "source": [
    "# LangGraph: Dynamically Returning a Tool Output Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "516dea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchTool(BaseModel):\n",
    "    query: str = Field(description=\"query to look up online\")\n",
    "    return_direct:bool = Field(\n",
    "        default=False,\n",
    "        description=\"whether or the result of this should be returned directly to the user without you seeing what it is\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d95943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Tải API key từ file .env\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "search_tool = [\n",
    "    TavilySearchResults(max_results=3,\n",
    "    api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "    args_schema=SearchTool)]\n",
    "\n",
    "tools = search_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_executor = ToolNode(tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ccaa583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama3-70b-8192\",\n",
    "     temperature=0,api_key=os.getenv(\"GROQ_API_KEY\") \n",
    "     )\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "model = model.bind_functions(functions)\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66b84649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentAction\n",
    "\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "  \n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        arguments = json.loads(last_message.additional_kwargs[\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "        if arguments.get(\"return_direct\", False):\n",
    "            return \"final\"\n",
    "        else:\n",
    "            return \"continue\"\n",
    "    \n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\" : [response]}\n",
    "\n",
    "\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]  # lấy call đầu tiên\n",
    "    tool_name = tool_call[\"function\"][\"name\"]\n",
    "    tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "    if tool_name == \"tavily_search_results_json\":\n",
    "        # Nếu là công cụ tìm kiếm, không cần gọi lại\n",
    "        if \"return_direct\" in tool_args:\n",
    "            del tool_args[\"return_direct\"]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    action = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=\"\",  # để trống nếu không cần\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=tool_name)\n",
    "    return {\"messages\": [function_message]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "469fc169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "workflow.add_node(\"final\", call_tool)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"final\": \"final\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.add_edge(\"final\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73cdb425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cz1j', 'function': {'arguments': '{\"query\":\"current weather in san francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 993, 'total_tokens': 1045, 'completion_time': 0.155212681, 'prompt_time': 0.04202971, 'queue_time': 0.247091823, 'total_time': 0.197242391}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5914505d-e8f1-4247-ad44-fb9d00500883-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in san francisco'}, 'id': 'call_cz1j', 'type': 'tool_call'}], usage_metadata={'input_tokens': 993, 'output_tokens': 52, 'total_tokens': 1045})]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'action':\n",
      "------------------\n",
      "{'messages': [FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json')]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_favg', 'function': {'arguments': '{\"query\":\"current weather in san francisco ca\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 1061, 'total_tokens': 1108, 'completion_time': 0.134285714, 'prompt_time': 0.036922454, 'queue_time': 0.24546927000000002, 'total_time': 0.171208168}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--77535d74-23b4-4e74-a668-3ec861a4dff8-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in san francisco ca'}, 'id': 'call_favg', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1061, 'output_tokens': 47, 'total_tokens': 1108})]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'action':\n",
      "------------------\n",
      "{'messages': [FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json')]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content=\"It seems like the tool is not providing the desired result. I'll respond directly without using a tool> The current weather in San Francisco is typically mild and cool, with fog rolling in from the Pacific Ocean. However, I'd recommend checking a weather website or app for the most up-to-date and accurate information.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 1128, 'total_tokens': 1192, 'completion_time': 0.313479608, 'prompt_time': 0.040561127, 'queue_time': 0.24708179700000002, 'total_time': 0.354040735}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--b88a0d8e-404e-4064-ade9-a78064ba9512-0', usage_metadata={'input_tokens': 1128, 'output_tokens': 64, 'total_tokens': 1192})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from node '{key}':\")\n",
    "        print(\"------------------\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35549b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vd67', 'function': {'arguments': '{\"query\":\"current weather in san francisco\",\"return_direct\":true}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 1003, 'total_tokens': 1084, 'completion_time': 0.231428571, 'prompt_time': 0.039899472, 'queue_time': 0.249101001, 'total_time': 0.271328043}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ca53507d-5e45-4b13-911c-ff1df2b00b08-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in san francisco', 'return_direct': True}, 'id': 'call_vd67', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1003, 'output_tokens': 81, 'total_tokens': 1084})]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'final':\n",
      "------------------\n",
      "{'messages': [FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results_json')]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf? return this result directly by setting return_direct=True\")]}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from node '{key}':\")\n",
    "        print(\"------------------\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e2412",
   "metadata": {},
   "source": [
    "## LangGraph: Respond in a Specific Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7d8b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xgt3', 'function': {'arguments': '{\"temperature\":0,\"other_notes\":\"Please provide the current temperature\"}', 'name': 'Response'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 1075, 'total_tokens': 1129, 'completion_time': 0.208917822, 'prompt_time': 0.035049803, 'queue_time': 0.278181148, 'total_time': 0.243967625}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6c436fee-5c2c-4f7a-a3bf-13c0596dfc3e-0', tool_calls=[{'name': 'Response', 'args': {'temperature': 0, 'other_notes': 'Please provide the current temperature'}, 'id': 'call_xgt3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1075, 'output_tokens': 54, 'total_tokens': 1129})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Tải API key từ file .env\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tools = [\n",
    "    TavilySearchResults(max_results=3)]\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_executor = ToolNode(tools)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama3-70b-8192\",\n",
    "     temperature=0,api_key=os.getenv(\"GROQ_API_KEY\") \n",
    "     )\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "class Response(BaseModel):\n",
    "    temperature: float = Field(description=\"the temperature\")\n",
    "    other_notes: str = Field(description=\"any other notes about the weather\")\n",
    "\n",
    "\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "functions.append(convert_pydantic_to_openai_function(Response))\n",
    "model = model.bind_functions(functions)\n",
    "\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "\n",
    "from langchain_core.agents import AgentAction\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "  \n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    elif last_message.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"] == \"Response\":\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\" : [response]}\n",
    "\n",
    "\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]  # lấy call đầu tiên\n",
    "    print(last_message.additional_kwargs[\"tool_calls\"])\n",
    "    tool_name = tool_call[\"function\"][\"name\"]\n",
    "    tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "    action = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=\"\",  # để trống nếu không cần\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=tool_name)\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from node '{key}':\")\n",
    "        print(\"------------------\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e265c",
   "metadata": {},
   "source": [
    "## Add managin Agent steps => add in call_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state):\n",
    "    messages = state[\"messages\"][-5:]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\" : [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfc336",
   "metadata": {},
   "source": [
    "## Force-calling a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75ab391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from node 'first_agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'cc0520af-0bab-46b1-8b66-6246ee00743e', 'type': 'function', 'function': {'name': 'tavily_search_results', 'arguments': '{\"query\": \"what is the weather in sf\"}'}}]}, response_metadata={}, tool_calls=[{'name': 'tavily_search_results', 'args': {'query': 'what is the weather in sf'}, 'id': 'cc0520af-0bab-46b1-8b66-6246ee00743e', 'type': 'tool_call'}])]}\n",
      "\n",
      "---\n",
      "\n",
      "[{'id': 'cc0520af-0bab-46b1-8b66-6246ee00743e', 'type': 'function', 'function': {'name': 'tavily_search_results', 'arguments': '{\"query\": \"what is the weather in sf\"}'}}]\n",
      "output from node 'action':\n",
      "------------------\n",
      "{'messages': [FunctionMessage(content=\"{'messages': []}\", additional_kwargs={}, response_metadata={}, name='tavily_search_results')]}\n",
      "\n",
      "---\n",
      "\n",
      "output from node 'agent':\n",
      "------------------\n",
      "{'messages': [AIMessage(content=\"It seems like the tool_calls didn't yield a useful result. In this case, I'll respond directly without using a tool.\\n\\nThe weather in San Francisco is cool and mild, with foggy mornings and sunny afternoons. However, it can vary depending on the time of year and other factors. If you're looking for a more specific to the current time, I can suggest checking a weather website or app for the most up-to-date information.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 1029, 'total_tokens': 1121, 'completion_time': 0.413598421, 'prompt_time': 0.033671931, 'queue_time': 0.27750274, 'total_time': 0.447270352}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--84332bf0-4ed4-429b-82aa-392401ecf4bd-0', usage_metadata={'input_tokens': 1029, 'output_tokens': 92, 'total_tokens': 1121})]}\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Tải API key từ file .env\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tools = [\n",
    "    TavilySearchResults(max_results=3)]\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_executor = ToolNode(tools)\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model_name=\"llama3-70b-8192\",\n",
    "     temperature=0,api_key=os.getenv(\"GROQ_API_KEY\") \n",
    "     )\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "\n",
    "\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "\n",
    "model = model.bind_functions(functions)\n",
    "\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "\n",
    "from langchain_core.agents import AgentAction\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "  \n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    elif last_message.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"] == \"Response\":\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\" : [response]}\n",
    "\n",
    "\n",
    "\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]  # lấy call đầu tiên\n",
    "    print(last_message.additional_kwargs[\"tool_calls\"])\n",
    "    tool_name = tool_call[\"function\"][\"name\"]\n",
    "    tool_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "    action = AgentAction(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_args,\n",
    "        log=\"\",  # để trống nếu không cần\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(content=str(response), name=tool_name)\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "#Modification\n",
    "from langchain_core.messages import AIMessage\n",
    "from uuid import uuid4\n",
    "def first_model(state):\n",
    "    human_input = state[\"messages\"][-1].content\n",
    "    return {\n",
    "        \"messages\":[\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                additional_kwargs={\n",
    "                    \"tool_calls\": [\n",
    "                        {\n",
    "                            \"id\": str(uuid4()),  # ✅ bắt buộc có id\n",
    "                            \"type\": \"function\",  # ✅ bắt buộc type\n",
    "                            \"function\": {\n",
    "                                \"name\": \"tavily_search_results\",  # ✅ đúng tên tool\n",
    "                                \"arguments\": json.dumps({\"query\": human_input})\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )           \n",
    "        ]\n",
    "    }\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"first_agent\", first_model)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"first_agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.add_edge(\"first_agent\", \"action\")\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"output from node '{key}':\")\n",
    "        print(\"------------------\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
